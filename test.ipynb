{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import add_self_loops\n",
    "import torch.nn as nn\n",
    "\n",
    "num_bond_type = 7 \n",
    "num_bond_direction = 3 \n",
    "\n",
    "class GINConv(MessagePassing):\n",
    "    def __init__(self, emb_dim, aggr=\"add\"):\n",
    "        super(GINConv, self).__init__()\n",
    "        self.mlp = nn.Sequential(nn.Linear(emb_dim, 2 * emb_dim), nn.ReLU(), nn.Linear(2 * emb_dim, emb_dim))\n",
    "        self.edge_embedding1 = nn.Embedding(num_bond_type, emb_dim)\n",
    "        self.edge_embedding2 = nn.Embedding(num_bond_direction, emb_dim)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.edge_embedding1.weight.data)\n",
    "        nn.init.xavier_uniform_(self.edge_embedding2.weight.data)\n",
    "        self.aggr = aggr\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        edge_index = add_self_loops(edge_index, num_nodes=x.size(0))\n",
    "\n",
    "        self_loop_attr = torch.zeros(x.size(0), 2)\n",
    "        self_loop_attr[:, 0] = 4  # bond type for self-loop edge\n",
    "        self_loop_attr = self_loop_attr.to(edge_attr.device).to(edge_attr.dtype)\n",
    "        edge_attr = torch.cat((edge_attr, self_loop_attr), dim=0)\n",
    "\n",
    "        edge_embeddings = self.edge_embedding1(edge_attr[:, 0]) + self.edge_embedding2(edge_attr[:, 1])\n",
    "        print(edge_attr[:, 0].shape, edge_attr[:, 1].shape)\n",
    "        print(self.edge_embedding1, self.edge_embedding2)\n",
    "        print(f\"Shape of edge embeddings: {edge_embeddings.shape}\")  # Tracking edge embeddings shape\n",
    "        print(f\"Shape of edge embedding 0: {self.edge_embedding1(edge_attr[:, 0])}, Shape of edge embedding 1: {self.edge_embedding2(edge_attr[:, 1])}\")  # Tracking edge embedding shapes\n",
    "        return self.propagate(edge_index[0], x=x, edge_attr=edge_embeddings)\n",
    "\n",
    "    def message(self, x_j, edge_attr):\n",
    "        print(f\"Shape of x_j in message: {x_j.shape}, Shape of edge_attr in message: {edge_attr.shape}\")  # Tracking shapes in message\n",
    "        return x_j + edge_attr\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        print(f\"Shape of aggr_out in update: {aggr_out.shape}\")  # Tracking shape in update\n",
    "        return self.mlp(aggr_out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7]) torch.Size([7])\n",
      "Embedding(7, 5) Embedding(3, 5)\n",
      "Shape of edge embeddings: torch.Size([7, 5])\n",
      "Shape of edge embedding 0: tensor([[-0.1417, -0.5789, -0.1907, -0.0125,  0.2462],\n",
      "        [ 0.2160,  0.0211,  0.1449, -0.6272, -0.1873],\n",
      "        [-0.1180, -0.5317, -0.5951, -0.5017,  0.1155],\n",
      "        [-0.5059, -0.2013,  0.0832, -0.2865, -0.5461],\n",
      "        [ 0.5345,  0.3491,  0.3122,  0.1297,  0.1854],\n",
      "        [ 0.5345,  0.3491,  0.3122,  0.1297,  0.1854],\n",
      "        [ 0.5345,  0.3491,  0.3122,  0.1297,  0.1854]],\n",
      "       grad_fn=<EmbeddingBackward0>), Shape of edge embedding 1: tensor([[-0.4809,  0.7644, -0.5652,  0.3381, -0.8205],\n",
      "        [ 0.1137,  0.2010,  0.2618, -0.8248, -0.7822],\n",
      "        [ 0.4874, -0.6461, -0.6930, -0.8118,  0.1054],\n",
      "        [ 0.1137,  0.2010,  0.2618, -0.8248, -0.7822],\n",
      "        [-0.4809,  0.7644, -0.5652,  0.3381, -0.8205],\n",
      "        [-0.4809,  0.7644, -0.5652,  0.3381, -0.8205],\n",
      "        [-0.4809,  0.7644, -0.5652,  0.3381, -0.8205]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "Shape of x_j in message: torch.Size([7, 5]), Shape of edge_attr in message: torch.Size([7, 5])\n",
      "Shape of aggr_out in update: torch.Size([3, 5])\n",
      "Updated node features:\n",
      "tensor([[ 0.5812,  0.6582, -0.0542, -0.8419,  0.2379],\n",
      "        [ 0.3410,  0.2402,  0.0599, -0.4773, -0.2192],\n",
      "        [ 0.0146,  0.5745,  0.0791, -0.6567, -0.1080]],\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage\n",
    "emb_dim = 5\n",
    "gin_conv = GINConv(emb_dim=emb_dim)\n",
    "\n",
    "# Dummy data for testing\n",
    "num_nodes = 3\n",
    "x = torch.randn(num_nodes, emb_dim)  # Node features\n",
    "edge_index = torch.tensor([[0, 1, 2, 0], [1, 2, 0, 2]])  # Edges\n",
    "edge_attr = torch.tensor([[1, 0], [2, 1], [3, 2], [0, 1]])  # Edge attributes\n",
    "\n",
    "# Forward pass\n",
    "updated_node_features = gin_conv(x, edge_index, edge_attr)\n",
    "print(\"Updated node features:\")\n",
    "print(updated_node_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Linear, Parameter\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import add_self_loops, degree\n",
    "\n",
    "class GCNConv(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__(aggr='add')  # \"Add\" aggregation (Step 5).\n",
    "        self.out_channels = out_channels\n",
    "        self.lin = Linear(in_channels, out_channels, bias=False)\n",
    "        self.bias = Parameter(torch.empty(out_channels))\n",
    "        self.embeddings = torch.nn.Embedding(2, out_channels)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.lin.reset_parameters()\n",
    "        self.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        # x has shape [N, in_channels]\n",
    "        # edge_index has shape [2, E]\n",
    "\n",
    "        # Step 1: Add self-loops to the adjacency matrix.\n",
    "        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n",
    "        # print(f\"1. shape endge index {edge_index.shape}\")\n",
    "        #add features corresponding to self-loop edges.\n",
    "        self_loop_attr = torch.zeros(x.size(0), 19)\n",
    "        self_loop_attr[:,-1] = 1 #bond type for self-loop edge\n",
    "        self_loop_attr = self_loop_attr.to(edge_attr.device).to(edge_attr.dtype)\n",
    "        edge_attr = torch.cat((edge_attr, self_loop_attr), dim = 0)\n",
    "        \n",
    "        summed_embeddings = torch.zeros(edge_attr.size(0), self.out_channels)\n",
    "\n",
    "        for i in range(19):  # Iterate over the second dimension\n",
    "            summed_embeddings += self.embeddings(edge_attr[:, i])\n",
    "        # Step 2: Linearly transform node feature matrix.\n",
    "        x = self.lin(x)\n",
    "        # print(f\"2. shape x {x.shape}\")\n",
    "        # Step 3: Compute normalization.\n",
    "        row, col = edge_index\n",
    "        deg = degree(col, x.size(0), dtype=x.dtype)\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
    "        # print(f\"3. shape norm {norm.shape}\")\n",
    "        # Step 4-5: Start propagating messages.\n",
    "        out = self.propagate(edge_index, x=x, edge_attr = summed_embeddings,norm=norm)\n",
    "        # print(f\"7. shape out {out.shape}\")\n",
    "        # Step 6: Apply a final bias vector.\n",
    "        out = out + self.bias\n",
    "\n",
    "        return out\n",
    "\n",
    "    def message(self, x_j, edge_attr, norm):\n",
    "        # x_j has shape [E, out_channels]\n",
    "\n",
    "        # Step 4: Normalize node features.\n",
    "        # print(f\"4. shape xj {x_j.shape}\")\n",
    "        # print(f\"5. shape norm.vier(-1,1) {norm.view(-1, 1).shape}\")\n",
    "        # print(f\"6. shape output {(norm.view(-1, 1) * x_j).shape}\")\n",
    "        return norm.view(-1, 1) * (x_j + edge_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original node features:\n",
      " tensor([[ 1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.],\n",
      "        [ 7.,  8.,  9.],\n",
      "        [10., 11., 12.]])\n",
      "\n",
      "Transformed node features:\n",
      " tensor([[ 10.4818, -20.3537],\n",
      "        [  9.5020, -15.6413],\n",
      "        [ 10.4818, -20.3537],\n",
      "        [  8.4351, -17.7163]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# Initialize the GCNConv layer\n",
    "# Transform from 3-dimensional features to 2-dimensional features\n",
    "gcn_conv = GCNConv(in_channels=3, out_channels=2)\n",
    "\n",
    "# Define node features (4 nodes with 3 features each)\n",
    "x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]], dtype=torch.float)\n",
    "\n",
    "# Define the edges in the graph (making it undirected)\n",
    "# Each pair of nodes is connected in both directions\n",
    "edge_index = torch.tensor([[0, 1, 1, 2, 2, 3, 3, 0, 0, 2], \n",
    "                           [1, 0, 2, 1, 3, 2, 0, 3, 2, 0]], dtype=torch.long)  # Edges: 0-1, 1-2, 2-3, 3-0, 0-2\n",
    "edge_attr = torch.randint(0, 1, (edge_index.size(1), 19))\n",
    "# Apply the GCNConv layer to the node features\n",
    "out_features = gcn_conv(x, edge_index, edge_attr)\n",
    "\n",
    "print(\"Original node features:\\n\", x)\n",
    "print(\"\\nTransformed node features:\\n\", out_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_scatter import scatter_add\n",
    "class GCNConv(MessagePassing):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, aggr = \"add\"):\n",
    "        super(GCNConv, self).__init__()\n",
    "\n",
    "        self.out_channels = out_channels\n",
    "        self.linear = torch.nn.Linear(in_channels, out_channels)\n",
    "        self.edge_embedding = torch.nn.Embedding(2, out_channels)\n",
    "\n",
    "        torch.nn.init.xavier_uniform_(self.edge_embedding.weight.data)\n",
    "\n",
    "        self.aggr = aggr\n",
    "\n",
    "    def norm(self, edge_index, num_nodes, dtype):\n",
    "        ### assuming that self-loops have been already added in edge_index\n",
    "        edge_weight = torch.ones((edge_index.size(1), ), dtype=dtype,\n",
    "                                     device=edge_index.device)\n",
    "        row, col = edge_index\n",
    "        deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes)\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "\n",
    "        return deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n",
    "\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        #add self loops in the edge space\n",
    "        edge_index, _ = add_self_loops(edge_index, num_nodes = x.size(0))\n",
    "\n",
    "        #add features corresponding to self-loop edges.\n",
    "        self_loop_attr = torch.zeros(x.size(0), 19)\n",
    "        self_loop_attr[:,-1] = 1 #bond type for self-loop edge\n",
    "        self_loop_attr = self_loop_attr.to(edge_attr.device).to(edge_attr.dtype)\n",
    "        edge_attr = torch.cat((edge_attr, self_loop_attr), dim = 0)\n",
    "\n",
    "        edge_embeddings = torch.zeros(edge_attr.size(0), self.out_channels)\n",
    "        for i in range(19):  # Iterate over the second dimension\n",
    "            edge_embeddings += self.edge_embedding(edge_attr[:, i])\n",
    "\n",
    "        norm = self.norm(edge_index, x.size(0), x.dtype)\n",
    "\n",
    "        x = self.linear(x)\n",
    "\n",
    "        return self.propagate(edge_index, x=x, edge_attr=edge_embeddings, norm=norm)\n",
    "\n",
    "    def message(self, x_j, edge_attr, norm):\n",
    "        return norm.view(-1, 1) * (x_j + edge_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 must have the same dtype",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/labhhc/Documents/Workspace/D19/Thinh/Thesis/Github/KGGraph/test.ipynb Cell 6\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/labhhc/Documents/Workspace/D19/Thinh/Thesis/Github/KGGraph/test.ipynb#X16sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m edge_attr \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, (\u001b[39m6\u001b[39m, \u001b[39m19\u001b[39m))  \u001b[39m# Edge attributes [6, 19]\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/labhhc/Documents/Workspace/D19/Thinh/Thesis/Github/KGGraph/test.ipynb#X16sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m gcn_conv \u001b[39m=\u001b[39m GCNConv(in_channels, out_channels)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/labhhc/Documents/Workspace/D19/Thinh/Thesis/Github/KGGraph/test.ipynb#X16sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m out_features \u001b[39m=\u001b[39m gcn_conv(x, edge_index, edge_attr)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/labhhc/Documents/Workspace/D19/Thinh/Thesis/Github/KGGraph/test.ipynb#X16sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mOutput node features:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/labhhc/Documents/Workspace/D19/Thinh/Thesis/Github/KGGraph/test.ipynb#X16sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mprint\u001b[39m(out_features)\n",
      "File \u001b[0;32m~/anaconda3/envs/Pyg/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/labhhc/Documents/Workspace/D19/Thinh/Thesis/Github/KGGraph/test.ipynb Cell 6\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/labhhc/Documents/Workspace/D19/Thinh/Thesis/Github/KGGraph/test.ipynb#X16sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     edge_embeddings \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39medge_embedding(edge_attr[:, i])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/labhhc/Documents/Workspace/D19/Thinh/Thesis/Github/KGGraph/test.ipynb#X16sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m norm \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm(edge_index, x\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), x\u001b[39m.\u001b[39mdtype)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/labhhc/Documents/Workspace/D19/Thinh/Thesis/Github/KGGraph/test.ipynb#X16sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlinear(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/labhhc/Documents/Workspace/D19/Thinh/Thesis/Github/KGGraph/test.ipynb#X16sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpropagate(edge_index, x\u001b[39m=\u001b[39mx, edge_attr\u001b[39m=\u001b[39medge_embeddings, norm\u001b[39m=\u001b[39mnorm)\n",
      "File \u001b[0;32m~/anaconda3/envs/Pyg/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/Pyg/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 must have the same dtype"
     ]
    }
   ],
   "source": [
    "in_channels = 42\n",
    "out_channels = 512  # Should match the second dimension of edge_attr\n",
    "x = torch.randint(0, 1, (4, in_channels))  # Node features [4, 4on-singleton dimension 0\n",
    "edge_attr = torch.randint(0, 1, (6, 19))  # Edge attributes [6, 19]\n",
    "\n",
    "gcn_conv = GCNConv(in_channels, out_channels)\n",
    "out_features = gcn_conv(x, edge_index, edge_attr)\n",
    "\n",
    "print(\"Output node features:\")\n",
    "print(out_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summed embeddings shape: torch.Size([14, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Simulating input tensor of shape [14, 19] with indices\n",
    "# Assuming each entry is an index, they should be integers (long type)\n",
    "# For simplicity, we're generating random indices here. In practice, these should be your actual data.\n",
    "input_tensor = torch.randint(0, 512, (14, 19), dtype=torch.long)\n",
    "\n",
    "# Create an embedding layer\n",
    "# Assuming maximum index is 511, thus embedding size is 512\n",
    "# Each index will map to a 512-dimensional vector\n",
    "embedding = nn.Embedding(512, 512)\n",
    "\n",
    "# Since Embedding expects a 1D input to map to a 1D output, we need to handle each \"feature\" separately\n",
    "# We can do this by iterating over the second dimension and summing or concatenating the embeddings\n",
    "# Here, we sum them for demonstration purposes\n",
    "\n",
    "# Initialize a tensor to store the sum of embeddings\n",
    "summed_embeddings = torch.zeros(14, 512)\n",
    "\n",
    "for i in range(19):  # Iterate over the second dimension\n",
    "    summed_embeddings += embedding(input_tensor[:, i])\n",
    "\n",
    "print(\"Summed embeddings shape:\", summed_embeddings.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kgg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
